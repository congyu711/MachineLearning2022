#logistic_regression.py

from cmath import e, exp, log
import math
import numpy as np


trN=59
tsN=32
xs=np.ones((trN,5))
ys=np.empty((trN,1))
cat=[]


with open('iris_train.data') as file:
    T=0
    for line1 in file:
        tmp=line1.split(',')
        for i in range(4):
            xs[T][i+1]=float(tmp[i])
        T=T+1
        cat.append(tmp[4])
    

beta=np.array([[5.21],[0.86],[-3.35],[0.34],[-1.842]])
# print(beta)
y=np.empty((trN,1))
for i in range(trN):
    if(cat[i]=='Iris-versicolor\n'): y[i,0]=1
    else: y[i,0]=0

# print(y)

def cost(beta,xs,y):
    res=0.0
    for i in range(trN):
        bx=xs[i,:].dot(beta)
        res-=y[i,0]*bx
        res+=math.log(1+math.exp(bx))
    return res

def _argmin(beta):
    N=50000
    alpha=0.001
    # tmp=beta
    for i in range(N):
        # alpha=random()/500
        cs=cost(beta,xs,y)
        gd=np.zeros((5,1))
        for i in range(trN):
            if(i%100==0): print(cs)
            xi=xs[i,:].reshape((1,5)).T
            gd-=y[i,0]*xi
            exb=math.exp(xi.T.dot(beta))
            gd+=xi*exb/(1+exb)
        # print(gd)
        if(cost(beta-gd*alpha,xs,y)>cs):
            print(beta)
            break
        beta-=gd*alpha
    return beta

beta=_argmin(beta)
print(beta)


# test.py

from cmath import e, exp, log
from random import betavariate, random
import math
import numpy as np


trN = 59
tsN = 32
xs = np.ones((tsN, 5))
ys = np.empty((tsN, 1))
cat = []


with open('iris_test.data') as file:
    T = 0
    for line1 in file:
        tmp = line1.split(',')
        for i in range(4):
            xs[T][i+1] = float(tmp[i])
        T = T+1
        if(tmp[4]=='Iris-setosa\n'): cat.append(0)
        elif(tmp[4]=='Iris-versicolor\n'): cat.append(1)
        else: cat.append(2)


beta = [np.array([[1.58142409],
                  [0.75021531],
                  [4.70227133],
                  [-7.54177246],
                  [0.33473693]]),
        np.array([[5.29674003],
                  [0.85383714],
                  [-3.36836161],
                  [0.34633183],
                  [-1.8437338]]),
        np.array([[-4.37580442],
                  [-6.65844692],
                  [-3.95858557],
                  [7.76941376],
                  [11.16229136]])
        ]

correct=0
for k in range(tsN):
    ls=[]
    for i in range(3):
        exb = xs[k, :].dot(beta[i])
        p = 1.0/(1+math.exp(-exb))
        ls.append(p)
        print(p, end=',')
    if(np.argmax(ls)==cat[k]): correct=correct+1
    print(np.argmax(ls))
    print(cat[k])
    print('\n')

print(correct/tsN)


# logistic_regression.out

Iris-setosa
[[ 1.58142409]
 [ 0.75021531]
 [ 4.70227133]
 [-7.54177246]
 [ 0.33473693]]

Iris-versicolor
[[ 5.29674003]
 [ 0.85383714]
 [-3.36836161]
 [ 0.34633183]
 [-1.8437338 ]]

Iris-virginica
[[-4.37580442]
 [-6.65844692]
 [-3.95858557]
 [ 7.76941376]
 [11.16229136]]

test.out
0.9998589483681922,0.20999566635049238,2.2117841972755255e-17,0
0


0.9999910796118204,0.04610534790002711,9.799520382314828e-17,0
0


0.9999994884224966,0.05097374093383698,8.960583986188033e-18,0
0


0.9997443314155726,0.14216948314541192,6.894399214253862e-15,0
0


0.9990010035249542,0.14579278713182167,5.683573867599893e-15,0
0


0.999412722748232,0.41246731021179295,7.107041448571098e-16,0
0


0.9999162241499323,0.11206033781208884,1.3600548052455377e-15,0
0


0.9999773376299405,0.12990245628875075,1.1921371179905696e-17,0
0


0.9999829395770887,0.16803493954948195,8.143742719505443e-18,0
0


4.831111226721027e-07,0.10902821561774935,0.7421542224919194,2
1


3.0194075735317075e-05,0.5155468507650673,2.7875446905298653e-05,1
1


1.0320632383281403e-08,0.7661029061157603,0.1967203904175672,1
1


1.488168934387943e-07,0.6198739203871714,0.002096523780295465,1
1


6.2992439653766085e-06,0.5213425201971557,2.618480937600467e-05,1
1


5.697325559377518e-06,0.44265667965545924,3.090127058197949e-05,1
1


1.2655000498673668e-07,0.6797418137720944,0.00040271078373090713,1
1


7.356261154756012e-08,0.3798171804362442,0.04564283354277352,1
1


1.1040018084715607e-06,0.36456770523067833,0.016293118408079944,1
1


0.0003428379190471198,0.6844501509663058,6.372935450328332e-07,1
1


1.2404172596367862e-05,0.7679425953820339,0.00016729658299812197,1
1


9.457488863513394e-10,0.8781927021567981,0.9279175076525769,2
2


1.3637465625599813e-09,0.13504876840906516,0.9990634461727295,2
2


2.9578394029602582e-08,0.2069143757383711,0.9995230221325638,2
2


1.8178351006387769e-13,0.7451345716012888,0.9995240727980417,2
2


2.9224878844021493e-08,0.4899120671510828,0.7595201987592918,2
2


1.7567621411159713e-09,0.1196347008170537,0.996583577727313,2
2


1.5037849377564246e-10,0.360056351065356,0.8487089618881952,2
2


9.224085513745962e-08,0.378196490266473,0.6554528841728547,2
2


1.0309579709330406e-07,0.22764917966608456,0.7848512449319622,2
2


2.840654560472158e-10,0.3537725100120505,0.9998602652188708,2
2


2.481685736423569e-10,0.5982150097513605,0.21923791410013826,1
2


1.2956354786863657e-11,0.6885292357194567,0.9795563270052792,2
2


0.9375